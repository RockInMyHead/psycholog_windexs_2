import { useState, useRef, useCallback, useEffect } from 'react';
import { useDeviceProfile } from './useDeviceProfile';
import { useAudioCapture } from './useAudioCapture';
import { useVAD } from './useVAD';
import { useBrowserSTT } from './useBrowserSTT';
import { useOpenAISTT } from './useOpenAISTT';
import { useTTSEchoGuard } from './useTTSEchoGuard';
import { useSTTTextProcessor } from './useSTTTextProcessor';

interface UseTranscriptionProps {
  onTranscriptionComplete: (text: string, source: 'browser' | 'openai' | 'manual') => void;
  onSpeechStart?: () => void;
  onInterruption?: () => void; // Called when user interrupts via voice
  isTTSActiveRef: React.MutableRefObject<boolean>; // To check if TTS is playing for echo cancellation
  onError?: (error: string) => void;
}

export const useTranscription = ({
  onTranscriptionComplete,
  onSpeechStart,
  onInterruption,
  isTTSActiveRef,
  onError,
  addDebugLog = console.log
}: UseTranscriptionProps & { addDebugLog?: (message: string) => void }) => {
  // Device detection (synchronously initialized, never null)
  const { profile: deviceProfile, getTranscriptionStrategy, shouldForceOpenAI } = useDeviceProfile();

  const [transcriptionStatus, setTranscriptionStatus] = useState<string | null>(null);
  const [transcriptionMode, setTranscriptionMode] = useState<'browser' | 'openai'>('browser');
  const [microphoneAccessGranted, setMicrophoneAccessGranted] = useState(false);
  const [microphonePermissionStatus, setMicrophonePermissionStatus] = useState<'unknown' | 'granted' | 'denied' | 'prompt'>('unknown');

  // TTS Echo Guard
  const ttsGuard = useTTSEchoGuard(deviceProfile);

  // Text processing
  const textProcessor = useSTTTextProcessor();

  // Audio capture
  const audioCapture = useAudioCapture(deviceProfile);

  // Voice Activity Detection
  const vad = useVAD(deviceProfile);

  // Browser STT
  const browserSTT = useBrowserSTT(
    deviceProfile,
    (text, isFinal) => {
      if (isFinal) {
        const normalized = textProcessor.normalizeSTT(text);
        if (normalized) {
          addDebugLog(`[User] ðŸŽ¤ ÐŸÐ¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ ÑÐºÐ°Ð·Ð°Ð»: "${normalized}" (browser)`);
          onTranscriptionComplete(normalized, 'browser');
        }
      }
    },
    (error) => {
      addDebugLog(`[BrowserSTT] Error: ${error}`);
      onError?.(error);
    },
    onInterruption,
    addDebugLog
  );

  // OpenAI STT
  const openaiSTT = useOpenAISTT(deviceProfile);

  // Mobile transcription timer
  const mobileTranscriptionTimerRef = useRef<number | null>(null);
  
  // P0-4: Ð›Ð¾Ðº Ð½Ð° Ñ€ÐµÑÑ‚Ð°Ñ€Ñ‚Ñ‹ recorder (Ð·Ð°Ñ‰Ð¸Ñ‚Ð° Ð¾Ñ‚ "ÑˆÑ‚Ð¾Ñ€Ð¼Ð¾Ð²")
  const restartLockRef = useRef(false);

  // --- Mobile Transcription Timer ---
  const startMobileTranscriptionTimer = useCallback(() => {
    if (mobileTranscriptionTimerRef.current) return;

    // P0-6: Platform-specific timer intervals (Ñ Ð·Ð°Ð¿Ð°ÑÐ¾Ð¼ Ð´Ð»Ñ iOS):
    // - iOS: 3500ms (Safari needs more time for stable chunks, Ñ‡Ñ‚Ð¾Ð±Ñ‹ chunkDuration=3000 ÑƒÑÐ¿ÐµÐ» Ð¿Ñ€Ð¸Ð»ÐµÑ‚ÐµÑ‚ÑŒ)
    // - Android: 2500ms (standard)
    // - Desktop: 2000ms (faster response on powerful devices)
    const timerInterval = deviceProfile.isIOS ? 3500 : deviceProfile.isAndroid ? 2500 : 2000;
    addDebugLog(`[Transcription] Starting timer (${timerInterval}ms interval) for ${deviceProfile.isIOS ? 'iOS' : deviceProfile.isAndroid ? 'Android' : 'Desktop'}`);

    mobileTranscriptionTimerRef.current = window.setInterval(async () => {
      const now = Date.now();

      // Don't process if TTS is active or in echo protection
      if (ttsGuard.shouldSuppressSTT(now)) {
        return;
      }

      try {
        // Check if recording is active and not paused
        addDebugLog(`[Timer] Recording state: isRecording=${audioCapture.state.isRecording}, isPaused=${audioCapture.state.isPaused}, chunks=${audioCapture.state.recordedChunks.length}`);

        // For continuous recording, we need accumulated chunks from the last timer interval
        // Instead of requestData (which may not work reliably), let's use accumulated chunks directly
        const blob = audioCapture.getAndClearChunks();
        addDebugLog(`[Timer] Got accumulated blob: ${blob?.size || 0} bytes`);

        // If no accumulated data, wait a bit more for chunks to arrive naturally
        let finalBlob = blob;
        if (!blob || blob.size === 0) {
          addDebugLog(`[Timer] No accumulated data, waiting 500ms for natural chunk accumulation`);
          await new Promise(resolve => setTimeout(resolve, 500));

          // Try to get chunks again
          finalBlob = audioCapture.getAndClearChunks();
          if (finalBlob && finalBlob.size > 0) {
            addDebugLog(`[Timer] Got data on retry: ${finalBlob.size} bytes`);
          } else {
            addDebugLog(`[Timer] Still no data after retry`);
          }
        }

        // Recording continues automatically without restart

        // P0-3: Check if we should send this audio (ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ Ñ timerInterval)
        const expectedMs = timerInterval;
        if (finalBlob && finalBlob.size > 0 && await vad.shouldSendAudio(finalBlob, expectedMs)) {
          addDebugLog(`[Timer] âœ… Sending blob (${finalBlob.size} bytes) for transcription`);
          setTranscriptionStatus("ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÑŽ Ð°ÑƒÐ´Ð¸Ð¾ Ð½Ð° ÑÐµÑ€Ð²ÐµÑ€...");
          const text = await openaiSTT.transcribeWithOpenAI(finalBlob);

          if (text) {
            const normalized = textProcessor.normalizeSTT(text);
            if (normalized) {
              addDebugLog(`[Mobile] âœ… Transcribed: "${normalized}"`);
              vad.markSendTime(now);
              onTranscriptionComplete(normalized, 'openai');
            }
          }
          setTranscriptionStatus("");
        } else {
          // If blob is empty or too small, just continue recording
          if (!blob || blob.size === 0) {
            addDebugLog(`[Timer] Empty blob, continuing recording`);
          }
        }
      } catch (error) {
        addDebugLog(`[Mobile] Error in timer: ${error}`);
        // P0-4: Check if recording is still active (Ñ Ð»Ð¾ÐºÐ¾Ð¼ Ð¿Ñ€Ð¾Ñ‚Ð¸Ð² ÑˆÑ‚Ð¾Ñ€Ð¼Ð¾Ð²)
        if (!audioCapture.state.isRecording && audioCapture.state.audioStream && !restartLockRef.current) {
          restartLockRef.current = true;
          try {
            await audioCapture.startRecording(audioCapture.state.audioStream);
            addDebugLog(`[Timer] Recording restarted after error`);
          } catch (restartError) {
            addDebugLog(`[Timer] Failed to restart recording: ${restartError}`);
          } finally {
            window.setTimeout(() => (restartLockRef.current = false), 1500);
          }
        }
      }
    }, timerInterval); // Platform-specific interval
  }, [deviceProfile.isIOS, deviceProfile.isAndroid, ttsGuard, audioCapture, vad, openaiSTT, textProcessor, onTranscriptionComplete, addDebugLog, setTranscriptionStatus]);

  const stopMobileTranscriptionTimer = useCallback(() => {
    if (mobileTranscriptionTimerRef.current) {
      addDebugLog(`[Mobile] Stopping transcription timer`);
      clearInterval(mobileTranscriptionTimerRef.current);
      mobileTranscriptionTimerRef.current = null;
    }
  }, [addDebugLog]);

  // --- Initialization ---
  const initializeRecognition = useCallback(async () => {
    addDebugLog(`[Init] ðŸš€ Starting recognition initialization...`);

    // Check microphone permissions
    if (navigator.permissions && navigator.permissions.query) {
      try {
        const result = await navigator.permissions.query({ name: 'microphone' as PermissionName });
        setMicrophonePermissionStatus(result.state);
        addDebugLog(`[Permissions] Microphone permission status: ${result.state}`);

        result.addEventListener('change', () => {
          setMicrophonePermissionStatus(result.state);
          addDebugLog(`[Permissions] Microphone permission changed to: ${result.state}`);
        });
      } catch (error) {
        addDebugLog(`[Permissions] Could not query microphone permissions: ${error}`);
      }
    } else {
      addDebugLog(`[Permissions] Permissions API not available`);
    }

    // Additional iOS diagnostics
    if (deviceProfile.isIOS) {
      addDebugLog(`[iOS Diagnostics] HTTPS: ${location.protocol === 'https:'}, Permissions API: ${!!navigator.permissions}, Secure Context: ${window.isSecureContext}`);
    }

    // Reset state
    textProcessor.clearDuplicates();
    vad.resetVADState();
    ttsGuard.setTTSActive(false, 0);

    // Get microphone stream with progressive fallback
    try {
      let stream: MediaStream;

      if (deviceProfile.isIOS) {
        // iOS progressive approach: try simple constraints first, then advanced
        addDebugLog(`[Mic] iOS: Trying simple constraints first...`);

        try {
          // Step 1: Try minimal constraints
          stream = await navigator.mediaDevices.getUserMedia({
            audio: { echoCancellation: false, noiseSuppression: false, autoGainControl: false }
          });
          addDebugLog(`[Mic] âœ… iOS simple constraints worked | Tracks: ${stream.getTracks().length}`);
        } catch (simpleError) {
          addDebugLog(`[Mic] âŒ iOS simple constraints failed: ${simpleError.message}, trying advanced...`);

          // Step 2: Try advanced constraints
          stream = await navigator.mediaDevices.getUserMedia({
            audio: {
              echoCancellation: true,
              noiseSuppression: true,
              autoGainControl: true,
              sampleRate: { ideal: 16000 },
              channelCount: { ideal: 1 }
            }
          });
          addDebugLog(`[Mic] âœ… iOS advanced constraints worked | Tracks: ${stream.getTracks().length}`);
        }
      } else {
        // Desktop: use full constraints directly
        const constraints = {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: { ideal: 44100 },
          channelCount: { ideal: 1 }
        };

        addDebugLog(`[Mic] Desktop: Requesting access with constraints...`);
        stream = await navigator.mediaDevices.getUserMedia({ audio: constraints });
        addDebugLog(`[Mic] âœ… Desktop access granted | Tracks: ${stream.getTracks().length}`);
      }

      // Additional iOS diagnostics and track validation
      const audioTracks = stream.getAudioTracks();

      if (deviceProfile.isIOS) {
        addDebugLog(`[iOS] Validating ${audioTracks.length} audio tracks...`);

        audioTracks.forEach((track, index) => {
          addDebugLog(`[iOS Track ${index}] enabled: ${track.enabled}, muted: ${track.muted}, readyState: ${track.readyState}, label: ${track.label}`);

          // Check if track is actually working
          if (!track.enabled) {
            addDebugLog(`[iOS] âš ï¸ Track ${index} is disabled, trying to enable...`);
            track.enabled = true;
          }

          // Monitor track state changes
          track.onended = () => addDebugLog(`[iOS] Track ${index} ended`);
          track.onmute = () => addDebugLog(`[iOS] Track ${index} muted`);
          track.onunmute = () => addDebugLog(`[iOS] Track ${index} unmuted`);
        });

        // iOS specific: wait a bit for tracks to stabilize
        await new Promise(resolve => setTimeout(resolve, 500));
      }

      // Final validation
      const activeTracks = audioTracks.filter(track => track.enabled && !track.muted);
      if (activeTracks.length === 0) {
        throw new Error('No active audio tracks available');
      }

      addDebugLog(`[Mic] âœ… Final validation passed | Active tracks: ${activeTracks.length}`);
      setMicrophoneAccessGranted(true);

      // Start volume monitoring for interruption detection (lightweight, doesn't conflict)
      vad.startVolumeMonitoring(stream, onInterruption);

      // Choose transcription strategy
      const forceOpenAI = shouldForceOpenAI(deviceProfile);
      
      // P0-1: iOS Ð²ÑÐµÐ³Ð´Ð° Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ OpenAI Ð´Ð»Ñ ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸
      const useOpenAI = deviceProfile.isIOS || forceOpenAI;

      addDebugLog(`[Strategy] Device: ${deviceProfile.isIOS ? 'iOS' : deviceProfile.isAndroid ? 'Android' : 'Desktop'} | Using: ${useOpenAI ? 'OpenAI' : 'Browser'}`);

      if (useOpenAI) {
        // OpenAI mode: needs MediaRecorder
        setTranscriptionMode('openai');
        
        // Ð’ÐÐ–ÐÐž: MediaRecorder ÑÑ‚Ð°Ñ€Ñ‚ÑƒÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð² OpenAI Ñ€ÐµÐ¶Ð¸Ð¼Ðµ
        await audioCapture.startRecording(stream);
        
        startMobileTranscriptionTimer();
        addDebugLog(`[Strategy] Started OpenAI mode with MediaRecorder`);
      } else {
        // Browser mode: ÐÐ• Ð·Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ MediaRecorder (Ð¸Ð·Ð±ÐµÐ³Ð°ÐµÐ¼ ÐºÐ¾Ð½Ñ„Ð»Ð¸ÐºÑ‚Ð¾Ð²)
        setTranscriptionMode('browser');
        browserSTT.start();
        addDebugLog(`[Strategy] Started Browser STT mode (no MediaRecorder)`);
      }

    } catch (error: any) {
      console.error('[Mic] âŒ Failed:', error);
      setMicrophoneAccessGranted(false);

      // Enhanced error handling for iOS with detailed diagnostics
      let userFriendlyErrorMessage = "ÐžÑˆÐ¸Ð±ÐºÐ° Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð° Ðº Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ñƒ";
      let diagnosticInfo = {
        errorName: error.name,
        errorMessage: error.message,
        isIOS: deviceProfile.isIOS,
        httpsEnabled: window.isSecureContext,
        permissionsAPISupported: !!navigator.permissions,
        userAgent: navigator.userAgent,
        timestamp: new Date().toISOString()
      };

      addDebugLog(`[Mic Error] Detailed diagnostics: ${JSON.stringify(diagnosticInfo, null, 2)}`);

      if (deviceProfile.isIOS) {
        // iOS-specific error handling with progressive solutions
        if (error.name === 'NotAllowedError') {
          userFriendlyErrorMessage = "Ð”Ð¾ÑÑ‚ÑƒÐ¿ Ðº Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ñƒ Ð·Ð°Ð¿Ñ€ÐµÑ‰ÐµÐ½. Ð¡Ð»ÐµÐ´ÑƒÐ¹Ñ‚Ðµ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸ÑÐ¼ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð²Ñ‹ÑˆÐµ. Ð•ÑÐ»Ð¸ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð° persists, Ð¿Ð¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ: 1) ÐŸÐµÑ€ÐµÐ·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ iPhone 2) ÐžÑ‡Ð¸ÑÑ‚Ð¸Ñ‚ÑŒ ÐºÑÑˆ Safari 3) Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð´Ñ€ÑƒÐ³Ð¾Ð¹ Ð±Ñ€Ð°ÑƒÐ·ÐµÑ€.";
        } else if (error.name === 'NotFoundError') {
          userFriendlyErrorMessage = "ÐœÐ¸ÐºÑ€Ð¾Ñ„Ð¾Ð½ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½ Ð¸Ð»Ð¸ Ð½Ðµ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚. ÐŸÑ€Ð¾Ð²ÐµÑ€ÑŒÑ‚Ðµ: 1) Ð Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð»Ð¸ Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½ Ð² Ð´Ñ€ÑƒÐ³Ð¸Ñ… Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸ÑÑ… (Ð”Ð¸ÐºÑ‚Ð¾Ñ„Ð¾Ð½) 2) ÐÐµÑ‚ Ð»Ð¸ Ñ„Ð¸Ð·Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð¿Ð¾Ð²Ñ€ÐµÐ¶Ð´ÐµÐ½Ð¸Ð¹ 3) ÐŸÐ¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½ Ð»Ð¸ Ð²Ð½ÐµÑˆÐ½Ð¸Ð¹ Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½.";
        } else if (error.name === 'NotReadableError') {
          userFriendlyErrorMessage = "ÐœÐ¸ÐºÑ€Ð¾Ñ„Ð¾Ð½ Ð·Ð°Ð½ÑÑ‚ ÑÐ¸ÑÑ‚ÐµÐ¼Ð¾Ð¹ iOS. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ: 1) ÐŸÐµÑ€ÐµÐ·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ iPhone 2) Ð—Ð°ÐºÑ€Ñ‹Ñ‚ÑŒ Ð²ÑÐµ Ð´Ñ€ÑƒÐ³Ð¸Ðµ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ 3) ÐŸÐµÑ€ÐµÐ·Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ Safari 4) Ð’Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾ Ð¾Ñ‚ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ Siri Ð¸ Ð´Ñ€ÑƒÐ³Ð¸Ðµ ÑÐµÑ€Ð²Ð¸ÑÑ‹.";
        } else if (error.name === 'SecurityError') {
          userFriendlyErrorMessage = "Ð¢Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ Ð·Ð°Ñ‰Ð¸Ñ‰ÐµÐ½Ð½Ð¾Ðµ ÑÐ¾ÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ðµ. Ð£Ð±ÐµÐ´Ð¸Ñ‚ÐµÑÑŒ Ñ‡Ñ‚Ð¾ Ð°Ð´Ñ€ÐµÑ Ð½Ð°Ñ‡Ð¸Ð½Ð°ÐµÑ‚ÑÑ Ñ 'https://' Ð¸ ÑÐ°Ð¹Ñ‚ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ SSL-ÑÐµÑ€Ñ‚Ð¸Ñ„Ð¸ÐºÐ°Ñ‚.";
        } else if (error.name === 'AbortError') {
          userFriendlyErrorMessage = "Ð—Ð°Ð¿Ñ€Ð¾Ñ Ð±Ñ‹Ð» Ð¿Ñ€ÐµÑ€Ð²Ð°Ð½. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ ÐµÑ‰Ðµ Ñ€Ð°Ð·. Ð•ÑÐ»Ð¸ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð° Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€ÑÐµÑ‚ÑÑ - Ð¿ÐµÑ€ÐµÐ·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñƒ.";
        } else if (error.name === 'NotSupportedError') {
          userFriendlyErrorMessage = "Ð’Ð°Ñˆ Ð±Ñ€Ð°ÑƒÐ·ÐµÑ€ Ð½Ðµ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÑ‚ Ð´Ð¾ÑÑ‚ÑƒÐ¿ Ðº Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ñƒ. ÐžÐ±Ð½Ð¾Ð²Ð¸Ñ‚Ðµ Safari Ð´Ð¾ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐ¹ Ð²ÐµÑ€ÑÐ¸Ð¸ Ð¸Ð»Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ Ð´Ñ€ÑƒÐ³Ð¾Ð¹ Ð±Ñ€Ð°ÑƒÐ·ÐµÑ€.";
        } else {
          userFriendlyErrorMessage = `ÐÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ð°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð° Ðº Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ñƒ Ð½Ð° iOS: ${error.message || 'Ð‘ÐµÐ· Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ñ'}. Ð ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÐ¼: 1) ÐŸÐµÑ€ÐµÐ·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ iPhone 2) ÐžÑ‡Ð¸ÑÑ‚Ð¸Ñ‚ÑŒ Ð´Ð°Ð½Ð½Ñ‹Ðµ Safari 3) ÐŸÐ¾Ð¿Ñ€Ð¾Ð±Ð¾Ð²Ð°Ñ‚ÑŒ Ð² Ð¿Ñ€Ð¸Ð²Ð°Ñ‚Ð½Ð¾Ð¼ Ñ€ÐµÐ¶Ð¸Ð¼Ðµ.`;
        }
      } else {
        // Generic error handling for other platforms
        if (error.name === 'NotAllowedError') {
          userFriendlyErrorMessage = "Ð”Ð¾ÑÑ‚ÑƒÐ¿ Ðº Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ñƒ Ð·Ð°Ð±Ð»Ð¾ÐºÐ¸Ñ€Ð¾Ð²Ð°Ð½ Ð±Ñ€Ð°ÑƒÐ·ÐµÑ€Ð¾Ð¼. Ð’ Ð°Ð´Ñ€ÐµÑÐ½Ð¾Ð¹ ÑÑ‚Ñ€Ð¾ÐºÐµ Ð±Ñ€Ð°ÑƒÐ·ÐµÑ€Ð° Ð½Ð°Ð¶Ð¼Ð¸Ñ‚Ðµ Ð½Ð° ðŸ”’ (Ð·Ð°Ð¼Ð¾Ñ‡ÐµÐº) Ð¸ Ñ€Ð°Ð·Ñ€ÐµÑˆÐ¸Ñ‚Ðµ Ð´Ð¾ÑÑ‚ÑƒÐ¿ Ðº Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ñƒ.";
        } else if (error.name === 'NotFoundError') {
          userFriendlyErrorMessage = "ÐœÐ¸ÐºÑ€Ð¾Ñ„Ð¾Ð½ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½. ÐŸÑ€Ð¾Ð²ÐµÑ€ÑŒÑ‚Ðµ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ð° Ð¸ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð·Ð²ÑƒÐºÐ°.";
        } else if (error.name === 'NotReadableError') {
          userFriendlyErrorMessage = "ÐœÐ¸ÐºÑ€Ð¾Ñ„Ð¾Ð½ Ð·Ð°Ð½ÑÑ‚ Ð´Ñ€ÑƒÐ³Ð¸Ð¼ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸ÐµÐ¼ Ð¸Ð»Ð¸ Ð²ÐºÐ»Ð°Ð´ÐºÐ¾Ð¹. Ð—Ð°ÐºÑ€Ð¾Ð¹Ñ‚Ðµ Ð´Ñ€ÑƒÐ³Ð¸Ðµ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ.";
        } else if (error.name === 'SecurityError') {
          userFriendlyErrorMessage = "Ð¢Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ Ð·Ð°Ñ‰Ð¸Ñ‰ÐµÐ½Ð½Ð¾Ðµ ÑÐ¾ÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ðµ (HTTPS) Ð´Ð»Ñ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð° Ðº Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ñƒ.";
        } else {
          userFriendlyErrorMessage = `ÐžÑˆÐ¸Ð±ÐºÐ° Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð° Ðº Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ñƒ: ${error.message || 'ÐÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ð°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ°'}. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ Ð¿ÐµÑ€ÐµÐ·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñƒ.`;
        }
      }

      onError?.(userFriendlyErrorMessage);
    }
  }, [
    deviceProfile, getTranscriptionStrategy, shouldForceOpenAI,
    audioCapture, vad, browserSTT, textProcessor, ttsGuard,
    onError, onInterruption, addDebugLog, startMobileTranscriptionTimer
  ]);

  // --- TTS Control ---
  const pauseRecordingForTTS = useCallback(() => {
    addDebugLog(`[TTS] PauseRecordingForTTS called, current state: isRecording=${audioCapture.state.isRecording}, isPaused=${audioCapture.state.isPaused}`);
    ttsGuard.setTTSActive(true, Date.now());

    // Stop volume monitoring
    vad.stopVolumeMonitoring();

    // P0-2: Ð’ OpenAI Ñ€ÐµÐ¶Ð¸Ð¼Ðµ - Ð¾ÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ Ñ‚Ð°Ð¹Ð¼ÐµÑ€ Ð¸ Ð¿Ð°ÑƒÐ·Ð¸Ð¼ Ð·Ð°Ð¿Ð¸ÑÑŒ
    if (transcriptionMode === 'openai') {
      stopMobileTranscriptionTimer();
      audioCapture.pauseRecording();
      addDebugLog(`[TTS] OpenAI mode: Timer stopped, recording paused`);
    }

    // Stop browser STT if active
    if (transcriptionMode === 'browser') {
      browserSTT.pause();
      addDebugLog(`[TTS] Browser STT paused`);
    }
  }, [ttsGuard, audioCapture, vad, browserSTT, transcriptionMode, stopMobileTranscriptionTimer, addDebugLog]);

  const resumeRecordingAfterTTS = useCallback(() => {
    const resumeDelay = ttsGuard.getResumeDelay();
    addDebugLog(`[TTS] Resume called, delay=${resumeDelay}ms`);

    window.setTimeout(async () => {
      ttsGuard.setTTSActive(false, Date.now());

      // P0-2: Restart volume monitoring
      if (audioCapture.state.audioStream) {
        vad.startVolumeMonitoring(audioCapture.state.audioStream, onInterruption);
      }

      // P0-2: OpenAI mode - resume Ð²Ð¼ÐµÑÑ‚Ð¾ restart
      if (transcriptionMode === 'openai') {
        // ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ð¿ÑƒÑ‚ÑŒ: resume Ð²Ð¼ÐµÑÑ‚Ð¾ restart
        if (audioCapture.state.isRecording && audioCapture.state.isPaused) {
          audioCapture.resumeRecording();
          addDebugLog(`[TTS] Recorder resumed`);
        } else if (!audioCapture.state.isRecording && audioCapture.state.audioStream) {
          // Ð¤Ð¾Ð»Ð»Ð±ÐµÐº: ÐµÑÐ»Ð¸ recorder Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ð»ÑÑ
          try {
            await audioCapture.startRecording(audioCapture.state.audioStream);
            addDebugLog(`[TTS] Recorder restarted (was not recording)`);
          } catch (e) {
            addDebugLog(`[TTS] Recorder restart failed: ${e}`);
          }
        }
        
        // ÐŸÐµÑ€ÐµÐ·Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ Ñ‚Ð°Ð¹Ð¼ÐµÑ€ ÐµÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð¾
        if (!mobileTranscriptionTimerRef.current) {
          startMobileTranscriptionTimer();
          addDebugLog(`[TTS] Timer restarted`);
        }
      }

      // Browser mode
      if (transcriptionMode === 'browser') {
        const extra = deviceProfile.isIOS ? 500 : 0;
        window.setTimeout(() => {
          browserSTT.resume();
          addDebugLog(`[TTS] Browser STT resumed`);
        }, resumeDelay + extra);
      }
    }, resumeDelay);
  }, [
    ttsGuard, audioCapture, vad, browserSTT,
    transcriptionMode, onInterruption, addDebugLog,
    deviceProfile.isIOS, startMobileTranscriptionTimer
  ]);

  // --- Cleanup ---
  const cleanup = useCallback((resetMicrophoneState: boolean = true) => {
    const callStack = new Error().stack;
    const caller = callStack?.split('\n')[2]?.trim() || 'unknown';
    addDebugLog(`[Transcription] ðŸ§¹ Cleanup called (resetMic: ${resetMicrophoneState}) - Called from: ${caller}`);

    try {
      // Safe cleanup - check if functions exist before calling
      if (mobileTranscriptionTimerRef.current) {
        clearInterval(mobileTranscriptionTimerRef.current);
        mobileTranscriptionTimerRef.current = null;
      }

      // Stop audio stream if it exists
      if (audioCapture?.state?.audioStream) {
        audioCapture.state.audioStream.getTracks().forEach(track => {
          try { track.stop(); } catch (e) { /* ignore */ }
        });
      }

      // Reset basic state
      setTranscriptionStatus(null);
      setTranscriptionMode('browser');

      // Only reset microphone access if explicitly requested
      if (resetMicrophoneState) {
        setMicrophoneAccessGranted(false);
      }

      addDebugLog(`[Transcription] ðŸ§¹ Cleanup completed successfully`);
    } catch (error) {
      addDebugLog(`[Transcription] Cleanup error (non-critical): ${error}`);
    }
  }, [addDebugLog]); // Only depend on addDebugLog

  // Cleanup on unmount - only when component actually unmounts
  useEffect(() => {
    return () => cleanup(true); // Full cleanup on unmount
  }, []); // Remove cleanup dependency to prevent excessive calls

  // Manual microphone access test for troubleshooting
  const testMicrophoneAccess = useCallback(async () => {
    try {
      addDebugLog(`[Mic Test] Manual microphone access test starting...`);

      const testStream = await navigator.mediaDevices.getUserMedia({
        audio: deviceProfile.isIOS ? { echoCancellation: false } : { echoCancellation: true }
      });

      const tracks = testStream.getAudioTracks();
      addDebugLog(`[Mic Test] âœ… Access successful | Tracks: ${tracks.length}`);

      // Log track details
      tracks.forEach((track, index) => {
        addDebugLog(`[Mic Test Track ${index}] label: ${track.label}, enabled: ${track.enabled}, muted: ${track.muted}`);
      });

      // Clean up test stream
      testStream.getTracks().forEach(track => track.stop());

      return { success: true, tracks: tracks.length };
    } catch (error: any) {
      addDebugLog(`[Mic Test] âŒ Access failed: ${error.name} - ${error.message}`);
      return { success: false, error: error.name, message: error.message };
    }
  }, [deviceProfile.isIOS, addDebugLog]);

  return {
    initializeRecognition,
    cleanup: (resetMicrophoneState = true) => cleanup(resetMicrophoneState),
    transcriptionStatus,
    microphoneAccessGranted,
    microphonePermissionStatus,
    isIOS: deviceProfile.isIOS,
    forceOpenAI: shouldForceOpenAI(deviceProfile),
    transcriptionMode,
    stopRecognition: browserSTT.stop,
    startRecognition: browserSTT.start,
    pauseRecordingForTTS,
    resumeRecordingAfterTTS,
    testMicrophoneAccess
  };
};
