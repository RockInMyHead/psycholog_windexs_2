import { useState, useRef, useEffect, useCallback } from 'react';
import { useDeviceProfile } from './useDeviceProfile';
import { useAudioCapture } from './useAudioCapture';
import { useVAD } from './useVAD';
import { useBrowserSTT } from './useBrowserSTT';
import { useOpenAISTT } from './useOpenAISTT';
import { useTTSEchoGuard } from './useTTSEchoGuard';
import { useSTTTextProcessor } from './useSTTTextProcessor';

interface UseTranscriptionProps {
  onTranscriptionComplete: (text: string, source: 'browser' | 'openai' | 'manual') => void;
  onSpeechStart?: () => void;
  onInterruption?: () => void; // Called when user interrupts via voice
  isTTSActiveRef: React.MutableRefObject<boolean>; // To check if TTS is playing for echo cancellation
  onError?: (error: string) => void;
}

export const useTranscription = ({
  onTranscriptionComplete,
  onSpeechStart,
  onInterruption,
  isTTSActiveRef,
  onError,
  addDebugLog = console.log
}: UseTranscriptionProps & { addDebugLog?: (message: string) => void }) => {
  // Device detection
  const { profile: deviceProfile, detectDevice, getTranscriptionStrategy, shouldForceOpenAI } = useDeviceProfile();

  // Initialize device profile
  useEffect(() => {
    detectDevice();
  }, [detectDevice]);

  const [transcriptionStatus, setTranscriptionStatus] = useState<string | null>(null);
  const [transcriptionMode, setTranscriptionMode] = useState<'browser' | 'openai'>('browser');
  const [microphoneAccessGranted, setMicrophoneAccessGranted] = useState(false);
  const [microphonePermissionStatus, setMicrophonePermissionStatus] = useState<'unknown' | 'granted' | 'denied' | 'prompt'>('unknown');

  // TTS Echo Guard
  const ttsGuard = useTTSEchoGuard(deviceProfile);

  // Text processing
  const textProcessor = useSTTTextProcessor();

  // Audio capture
  const audioCapture = useAudioCapture(deviceProfile);

  // Voice Activity Detection
  const vad = useVAD(deviceProfile);

  // Browser STT
  const browserSTT = useBrowserSTT(
    deviceProfile!,
    (text, isFinal) => {
      if (isFinal) {
        const normalized = textProcessor.normalizeSTT(text);
        if (normalized) {
          addDebugLog(`[User] ðŸŽ¤ ÐŸÐ¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ ÑÐºÐ°Ð·Ð°Ð»: "${normalized}" (browser)`);
          onTranscriptionComplete(normalized, 'browser');
        }
      }
    },
    (error) => {
      addDebugLog(`[BrowserSTT] Error: ${error}`);
      onError?.(error);
    },
    onInterruption
  );

  // OpenAI STT
  const openaiSTT = useOpenAISTT(deviceProfile);

  // Mobile transcription timer
  const mobileTranscriptionTimerRef = useRef<number | null>(null);

  // --- Mobile Transcription Timer ---
  const startMobileTranscriptionTimer = useCallback(() => {
    if (mobileTranscriptionTimerRef.current) return;

    addDebugLog(`[Mobile] Starting transcription timer (2s check interval)`);

    mobileTranscriptionTimerRef.current = window.setInterval(async () => {
      const now = Date.now();

      // Don't process if TTS is active or in echo protection
      if (ttsGuard.shouldSuppressSTT(now)) {
        return;
      }

      try {
        // Stop recording to get current accumulated audio
        const blob = await audioCapture.stopRecording();
        addDebugLog(`[Timer] Got accumulated blob: ${blob?.size || 0} bytes`);

        // IMMEDIATELY restart recording for next segment
        if (audioCapture.state.audioStream) {
          await audioCapture.startRecording(audioCapture.state.audioStream);
        }

        // Check if we should send this audio
        if (blob && await vad.shouldSendAudio(blob, 2000)) { // 2s duration
          setTranscriptionStatus("ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÑŽ Ð°ÑƒÐ´Ð¸Ð¾ Ð½Ð° ÑÐµÑ€Ð²ÐµÑ€...");
          const text = await openaiSTT.transcribeWithOpenAI(blob);

          if (text) {
            const normalized = textProcessor.normalizeSTT(text);
            if (normalized) {
              addDebugLog(`[Mobile] âœ… Transcribed: "${normalized}"`);
              vad.markSendTime(now);
              onTranscriptionComplete(normalized, 'openai');
            }
          }
          setTranscriptionStatus("");
        }
      } catch (error) {
        addDebugLog(`[Mobile] Error in timer: ${error}`);
        // Restart recording on error
        if (audioCapture.state.audioStream && !audioCapture.state.isRecording) {
          await audioCapture.startRecording(audioCapture.state.audioStream);
        }
      }
    }, 2000); // Check every 2 seconds
  }, [ttsGuard, audioCapture, vad, openaiSTT, textProcessor, onTranscriptionComplete, addDebugLog]);

  const stopMobileTranscriptionTimer = useCallback(() => {
    if (mobileTranscriptionTimerRef.current) {
      addDebugLog(`[Mobile] Stopping transcription timer`);
      clearInterval(mobileTranscriptionTimerRef.current);
      mobileTranscriptionTimerRef.current = null;
    }
  }, [addDebugLog]);

  // --- Initialization ---
  const initializeRecognition = useCallback(async () => {
    addDebugLog(`[Init] ðŸš€ Starting recognition initialization...`);

    // Check microphone permissions
    if (navigator.permissions && navigator.permissions.query) {
      try {
        const result = await navigator.permissions.query({ name: 'microphone' as PermissionName });
        setMicrophonePermissionStatus(result.state);
        addDebugLog(`[Permissions] Microphone permission status: ${result.state}`);

        result.addEventListener('change', () => {
          setMicrophonePermissionStatus(result.state);
          addDebugLog(`[Permissions] Microphone permission changed to: ${result.state}`);
        });
      } catch (error) {
        addDebugLog(`[Permissions] Could not query microphone permissions: ${error}`);
      }
    } else {
      addDebugLog(`[Permissions] Permissions API not available`);
    }

    // Additional iOS diagnostics
    if (deviceProfile.isIOS) {
      addDebugLog(`[iOS Diagnostics] HTTPS: ${location.protocol === 'https:'}, Permissions API: ${!!navigator.permissions}, Secure Context: ${window.isSecureContext}`);
    }

    // Reset state
    textProcessor.clearDuplicates();
    vad.resetVADState();
    ttsGuard.setTTSActive(false, 0);

    // Get microphone stream
    try {
      const constraints = {
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true,
        sampleRate: deviceProfile.isIOS ? { ideal: 16000 } : { ideal: 44100 },
        channelCount: { ideal: 1 }
      };

      addDebugLog(`[Mic] Requesting access...`);
      const stream = await navigator.mediaDevices.getUserMedia({ audio: constraints });
      addDebugLog(`[Mic] âœ… Access granted | Tracks: ${stream.getTracks().length}`);

      setMicrophoneAccessGranted(true);

      // Start audio capture
      await audioCapture.startRecording(stream);

      // Start volume monitoring for interruption detection
      vad.startVolumeMonitoring(stream, onInterruption);

      // Choose transcription strategy
      const strategy = getTranscriptionStrategy(deviceProfile);
      const forceOpenAI = shouldForceOpenAI(deviceProfile);

      addDebugLog(`[Strategy] ${strategy} | Force OpenAI: ${forceOpenAI}`);

      if (forceOpenAI) {
        // Android or forced OpenAI mode
        setTranscriptionMode('openai');
        startMobileTranscriptionTimer();
      } else {
        // iOS starts with browser mode, Android uses OpenAI
        if (deviceProfile.isIOS) {
          setTranscriptionMode('browser');
          browserSTT.start();
        } else {
          setTranscriptionMode('openai');
          startMobileTranscriptionTimer();
        }
      }

    } catch (error: any) {
      console.error('[Mic] âŒ Failed:', error);
      setMicrophoneAccessGranted(false);

      // Enhanced error handling for iOS
      let userFriendlyErrorMessage = "ÐžÑˆÐ¸Ð±ÐºÐ° Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð° Ðº Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ñƒ";

      if (deviceProfile.isIOS) {
        // iOS-specific error handling
        if (error.name === 'NotAllowedError') {
          userFriendlyErrorMessage = "Ð”Ð¾ÑÑ‚ÑƒÐ¿ Ðº Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ñƒ Ð·Ð°Ð¿Ñ€ÐµÑ‰ÐµÐ½. Ð’ Safari Ð¾Ñ‚ÐºÑ€Ð¾Ð¹Ñ‚Ðµ ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ > ÐšÐ¾Ð½Ñ„Ð¸Ð´ÐµÐ½Ñ†Ð¸Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚ÑŒ > ÐœÐ¸ÐºÑ€Ð¾Ñ„Ð¾Ð½ Ð¸ Ñ€Ð°Ð·Ñ€ÐµÑˆÐ¸Ñ‚Ðµ Ð´Ð¾ÑÑ‚ÑƒÐ¿ Ð´Ð»Ñ ÑÑ‚Ð¾Ð³Ð¾ ÑÐ°Ð¹Ñ‚Ð°.";
        } else if (error.name === 'NotFoundError') {
          userFriendlyErrorMessage = "ÐœÐ¸ÐºÑ€Ð¾Ñ„Ð¾Ð½ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½. Ð£Ð±ÐµÐ´Ð¸Ñ‚ÐµÑÑŒ Ñ‡Ñ‚Ð¾ Ñƒ ÑƒÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ð° ÐµÑÑ‚ÑŒ Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½ Ð¸ Ð¾Ð½ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚.";
        } else if (error.name === 'NotReadableError') {
          userFriendlyErrorMessage = "ÐœÐ¸ÐºÑ€Ð¾Ñ„Ð¾Ð½ Ð·Ð°Ð½ÑÑ‚ Ð´Ñ€ÑƒÐ³Ð¸Ð¼ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸ÐµÐ¼. Ð—Ð°ÐºÑ€Ð¾Ð¹Ñ‚Ðµ Ð´Ñ€ÑƒÐ³Ð¸Ðµ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‰Ð¸Ðµ Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½, Ð¸ Ð¿ÐµÑ€ÐµÐ·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñƒ.";
        } else if (error.name === 'SecurityError') {
          userFriendlyErrorMessage = "Ð¢Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ Ð·Ð°Ñ‰Ð¸Ñ‰ÐµÐ½Ð½Ð¾Ðµ ÑÐ¾ÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ðµ (HTTPS) Ð´Ð»Ñ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð° Ðº Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ñƒ.";
        } else if (error.name === 'AbortError') {
          userFriendlyErrorMessage = "Ð”Ð¾ÑÑ‚ÑƒÐ¿ Ðº Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ñƒ Ð±Ñ‹Ð» Ð¿Ñ€ÐµÑ€Ð²Ð°Ð½. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ ÐµÑ‰Ðµ Ñ€Ð°Ð·.";
        } else {
          userFriendlyErrorMessage = `ÐžÑˆÐ¸Ð±ÐºÐ° Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð° Ðº Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ñƒ Ð½Ð° iOS: ${error.message || 'ÐÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ð°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ°'}. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ Ð¿ÐµÑ€ÐµÐ·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Safari Ð¸Ð»Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð´Ñ€ÑƒÐ³Ð¾Ð¹ Ð±Ñ€Ð°ÑƒÐ·ÐµÑ€.`;
        }

        // Log additional iOS diagnostic info
        console.log('[iOS Mic Diagnostics]', {
          errorName: error.name,
          errorMessage: error.message,
          httpsRequired: !window.isSecureContext,
          userAgent: navigator.userAgent,
          permissionsAPI: !!navigator.permissions
        });
      } else {
        // Generic error handling for other platforms
        if (error.name === 'NotAllowedError') {
          userFriendlyErrorMessage = "Ð”Ð¾ÑÑ‚ÑƒÐ¿ Ðº Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ñƒ Ð·Ð°Ð¿Ñ€ÐµÑ‰ÐµÐ½. Ð Ð°Ð·Ñ€ÐµÑˆÐ¸Ñ‚Ðµ Ð´Ð¾ÑÑ‚ÑƒÐ¿ Ð² Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ°Ñ… Ð±Ñ€Ð°ÑƒÐ·ÐµÑ€Ð°.";
        } else if (error.name === 'NotFoundError') {
          userFriendlyErrorMessage = "ÐœÐ¸ÐºÑ€Ð¾Ñ„Ð¾Ð½ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½. ÐŸÑ€Ð¾Ð²ÐµÑ€ÑŒÑ‚Ðµ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ð°.";
        } else if (error.name === 'NotReadableError') {
          userFriendlyErrorMessage = "ÐœÐ¸ÐºÑ€Ð¾Ñ„Ð¾Ð½ Ð·Ð°Ð½ÑÑ‚ Ð´Ñ€ÑƒÐ³Ð¸Ð¼ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸ÐµÐ¼ Ð¸Ð»Ð¸ Ð²ÐºÐ»Ð°Ð´ÐºÐ¾Ð¹.";
        } else if (error.name === 'SecurityError') {
          userFriendlyErrorMessage = "Ð¢Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ Ð·Ð°Ñ‰Ð¸Ñ‰ÐµÐ½Ð½Ð¾Ðµ ÑÐ¾ÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ðµ (HTTPS) Ð´Ð»Ñ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð° Ðº Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ñƒ.";
        } else {
          userFriendlyErrorMessage = `ÐžÑˆÐ¸Ð±ÐºÐ° Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð° Ðº Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ñƒ: ${error.message || 'ÐÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ð°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ°'}`;
        }
      }

      onError?.(userFriendlyErrorMessage);
    }
  }, [
    deviceProfile, getTranscriptionStrategy, shouldForceOpenAI,
    audioCapture, vad, browserSTT, textProcessor, ttsGuard,
    onError, onInterruption, addDebugLog, startMobileTranscriptionTimer
  ]);

  // --- TTS Control ---
  const pauseRecordingForTTS = useCallback(() => {
    addDebugLog(`[TTS] Pausing recording for TTS`);
    ttsGuard.setTTSActive(true, Date.now());

    // Pause audio capture
    audioCapture.pauseRecording();

    // Stop volume monitoring
    vad.stopVolumeMonitoring();

    // Stop browser STT if active
    if (transcriptionMode === 'browser') {
      browserSTT.pause();
    }
  }, [ttsGuard, audioCapture, vad, browserSTT, transcriptionMode, addDebugLog]);

  const resumeRecordingAfterTTS = useCallback(() => {
    const resumeDelay = ttsGuard.getResumeDelay();
    addDebugLog(`[TTS] Resuming after TTS with ${resumeDelay}ms delay`);

    setTimeout(() => {
      ttsGuard.setTTSActive(false, Date.now());

      // Resume audio capture
      if (audioCapture.state.audioStream && audioCapture.state.isPaused) {
        audioCapture.resumeRecording();
      }

      // Restart volume monitoring
      if (audioCapture.state.audioStream) {
        vad.startVolumeMonitoring(audioCapture.state.audioStream, onInterruption);
      }

      // Resume appropriate transcription
      if (transcriptionMode === 'browser') {
        browserSTT.resume();
      }
      // Mobile timer continues automatically
    }, resumeDelay);
  }, [ttsGuard, audioCapture, vad, browserSTT, transcriptionMode, onInterruption, addDebugLog]);

  // --- Cleanup ---
  const cleanup = useCallback(() => {
    addDebugLog('[Transcription] ðŸ§¹ Cleanup called');

    // Stop everything
    stopMobileTranscriptionTimer();
    browserSTT.stop();
    vad.stopVolumeMonitoring();
    audioCapture.cleanup();

    // Reset state
    textProcessor.clearDuplicates();
    vad.resetVADState();
    ttsGuard.setTTSActive(false, 0);

    setTranscriptionStatus(null);
    setTranscriptionMode('browser');
    setMicrophoneAccessGranted(false);
  }, [
    stopMobileTranscriptionTimer, browserSTT, vad, audioCapture,
    textProcessor, ttsGuard, addDebugLog
  ]);

  // Cleanup on unmount
  useEffect(() => {
    return cleanup;
  }, [cleanup]);

  return {
    initializeRecognition,
    cleanup,
    transcriptionStatus,
    microphoneAccessGranted,
    microphonePermissionStatus,
    isIOS: deviceProfile.isIOS,
    forceOpenAI: shouldForceOpenAI(deviceProfile),
    transcriptionMode,
    stopRecognition: browserSTT.stop,
    startRecognition: browserSTT.start,
    pauseRecordingForTTS,
    resumeRecordingAfterTTS
  };
};
