import { useState, useEffect, useRef } from "react";
import { Button } from "@/components/ui/button";
import { Card } from "@/components/ui/card";
import { Phone, PhoneOff, Mic, MicOff, Volume2, VolumeX } from "lucide-react";
import Navigation from "@/components/Navigation";
import { userApi, audioCallApi, memoryApi, subscriptionApi } from "@/services/api";
import { useAuth } from "@/contexts/AuthContext";
import { psychologistAI, type ChatMessage } from "@/services/openai";

const AudioCall = () => {
  const { user: authUser } = useAuth();
  const [isCallActive, setIsCallActive] = useState(false);
  const [isMuted, setIsMuted] = useState(false);
  const [isSpeakerOn, setIsSpeakerOn] = useState(true);
  const [callDuration, setCallDuration] = useState(0);
  const [user, setUser] = useState<UserType | null>(null);
  const [currentCallId, setCurrentCallId] = useState<string | null>(null);
  const [transcriptionStatus, setTranscriptionStatus] = useState<string | null>(null);
  const [audioError, setAudioError] = useState<string | null>(null);
  const [loading, setLoading] = useState(true);
  const [fastMode, setFastMode] = useState(false); // –ë—ã—Å—Ç—Ä—ã–π —Ä–µ–∂–∏–º –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
  const [callType, setCallType] = useState<'psychologist' | 'teacher'>('psychologist'); // –¢–∏–ø –∑–≤–æ–Ω–∫–∞
  const [subscriptionInfo, setSubscriptionInfo] = useState<{ plan: 'premium' | 'free' | 'none'; remaining: number; limit: number; status: 'active' | 'inactive' | 'cancelled' | 'none' } | null>(null);

  const audioStreamRef = useRef<MediaStream | null>(null);
  const callTimerRef = useRef<number | null>(null);
  const recognitionRef = useRef<any>(null);
  const recognitionActiveRef = useRef(false);
  const isMutedRef = useRef(false);
  const isSpeakerOnRef = useRef(true);
  const conversationRef = useRef<ChatMessage[]>([]);
  const responseQueueRef = useRef<Promise<void>>(Promise.resolve());
  const audioQueueRef = useRef<ArrayBuffer[]>([]);
  const isPlayingAudioRef = useRef(false);
  const currentSpeechSourceRef = useRef<AudioBufferSourceNode | null>(null);
  const processingSoundIntervalRef = useRef<number | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const speakerGainRef = useRef<GainNode | null>(null);
  const backgroundMusicRef = useRef<{
    audioElement?: HTMLAudioElement;
    isPlaying: boolean;
  }>({ isPlaying: false });
  const callLimitReachedRef = useRef(false);
  const callLimitWarningSentRef = useRef(false);
  const memoryRef = useRef<string>("");
  const pendingTranscriptRef = useRef<string>("");
  const pendingProcessTimeoutRef = useRef<number | null>(null);
  const audioAnalyserRef = useRef<AnalyserNode | null>(null);
  const volumeMonitorRef = useRef<number | null>(null);

  const MAX_CALL_DURATION_SECONDS = 40 * 60;
  const CALL_LIMIT_WARNING_SECONDS = MAX_CALL_DURATION_SECONDS - 5 * 60;
  const VOICE_DETECTION_THRESHOLD = 20; // –£–º–µ–Ω—å—à–∏–ª–∏ –ø–æ—Ä–æ–≥ —Å 30 –¥–æ 20 –¥–ª—è –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä–æ–≥–æ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è

  const initializeAudioContext = () => {
    if (!audioContextRef.current) {
      audioContextRef.current = new AudioContext();
    }
    if (!speakerGainRef.current && audioContextRef.current) {
      const gainNode = audioContextRef.current.createGain();
      gainNode.gain.value = isSpeakerOnRef.current ? 1 : 0;
      gainNode.connect(audioContextRef.current.destination);
      speakerGainRef.current = gainNode;
    }
    return audioContextRef.current;
  };

  const getAudioOutputNode = () => {
    const audioContext = initializeAudioContext();
    if (!audioContext) {
      return null;
    }
    if (!speakerGainRef.current) {
      const gainNode = audioContext.createGain();
      gainNode.gain.value = isSpeakerOnRef.current ? 1 : 0;
      gainNode.connect(audioContext.destination);
      speakerGainRef.current = gainNode;
    }
    return speakerGainRef.current;
  };

  const playBeepSound = async (frequency: number = 800, duration: number = 200) => {
    try {
      const audioContext = initializeAudioContext();
      const outputNode = getAudioOutputNode();
      const oscillator = audioContext.createOscillator();
      const gainNode = audioContext.createGain();

      oscillator.connect(gainNode);
      gainNode.connect(outputNode ?? audioContext.destination);

      oscillator.frequency.setValueAtTime(frequency, audioContext.currentTime);
      oscillator.type = 'sine';

      gainNode.gain.setValueAtTime(0.1, audioContext.currentTime);
      gainNode.gain.exponentialRampToValueAtTime(0.01, audioContext.currentTime + duration / 1000);

      oscillator.start(audioContext.currentTime);
      oscillator.stop(audioContext.currentTime + duration / 1000);
    } catch (error) {
      console.warn('Could not play beep sound:', error);
    }
  };

  const startProcessingSound = () => {
    if (processingSoundIntervalRef.current) {
      clearInterval(processingSoundIntervalRef.current);
    }

    // –ü–µ—Ä–≤—ã–π —Å–∏–≥–Ω–∞–ª —Å—Ä–∞–∑—É
    playBeepSound(800, 150);

    // –ó–∞—Ç–µ–º –ø–æ–≤—Ç–æ—Ä—è—Ç—å –∫–∞–∂–¥—ã–µ 3 —Å–µ–∫—É–Ω–¥—ã
    processingSoundIntervalRef.current = window.setInterval(() => {
      playBeepSound(800, 150);
    }, 3000);
  };

  const stopProcessingSound = () => {
    if (processingSoundIntervalRef.current) {
      clearInterval(processingSoundIntervalRef.current);
      processingSoundIntervalRef.current = null;
    }
  };

  const startBackgroundMusic = async () => {
    if (backgroundMusicRef.current.isPlaying) {
      return; // –£–∂–µ –∏–≥—Ä–∞–µ—Ç
    }

    try {
      const audioElement = new Audio('/de144d31b1f3b3f.mp3');

      // –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∞—É–¥–∏–æ
      audioElement.loop = true; // –ó–∞—Ü–∏–∫–ª–∏–≤–∞–µ–º –º—É–∑—ã–∫—É
      audioElement.volume = 0.1; // –¢–∏—Ö–∞—è –≥—Ä–æ–º–∫–æ—Å—Ç—å (10%)
      audioElement.muted = !isSpeakerOnRef.current;
      audioElement.preload = 'auto';

      // –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–æ–±—ã—Ç–∏–π
      audioElement.addEventListener('canplaythrough', () => {
        audioElement.play().catch(error => {
          console.warn('Could not play background music:', error);
        });
      });

      audioElement.addEventListener('error', (error) => {
        console.warn('Background music loading error:', error);
      });

      backgroundMusicRef.current = {
        audioElement,
        isPlaying: true,
      };

    } catch (error) {
      console.warn('Could not start background music:', error);
    }
  };

  const stopBackgroundMusic = () => {
    if (!backgroundMusicRef.current.isPlaying) {
      return;
    }

    try {
      const { audioElement } = backgroundMusicRef.current;

      if (audioElement) {
        // –ü–ª–∞–≤–Ω–æ–µ –∑–∞—Ç—É—Ö–∞–Ω–∏–µ
        const fadeOut = () => {
          if (audioElement.volume > 0.01) {
            audioElement.volume -= 0.01;
            setTimeout(fadeOut, 50);
          } else {
            audioElement.pause();
            audioElement.currentTime = 0;
            backgroundMusicRef.current.isPlaying = false;
          }
        };
        fadeOut();
      } else {
        backgroundMusicRef.current.isPlaying = false;
      }

    } catch (error) {
      console.warn('Could not stop background music:', error);
      backgroundMusicRef.current.isPlaying = false;
    }
  };

  const splitIntoSentences = (text: string): string[] => {
    return text
      .split(/(?<=[.!?])\s+/u)
      .map((sentence) => sentence.trim())
      .filter((sentence) => sentence.length > 0);
  };

  const resetAudioPlayback = () => {
    audioQueueRef.current = [];
    isPlayingAudioRef.current = false;
    if (audioContextRef.current) {
      audioContextRef.current.close().catch((error) => {
        console.warn("Error closing AudioContext:", error);
      });
      audioContextRef.current = null;
    }
    speakerGainRef.current = null;
  };

  const playQueuedAudio = async () => {
    console.log("[AudioCall] playQueuedAudio called, queue length:", audioQueueRef.current.length);
    if (isPlayingAudioRef.current || audioQueueRef.current.length === 0) {
      console.log("[AudioCall] Already playing or queue empty, skipping");
      return;
    }

    if (!audioContextRef.current) {
      console.log("[AudioCall] Creating AudioContext");
      const AudioContextConstructor = window.AudioContext || (window as any).webkitAudioContext;
      audioContextRef.current = new AudioContextConstructor();
    }

    const audioContext = audioContextRef.current;
    if (!audioContext) {
      console.warn("[AudioCall] AudioContext unavailable.");
      audioQueueRef.current = [];
      return;
    }

    const outputNode = getAudioOutputNode();
    console.log("[AudioCall] Output node:", outputNode);

    isPlayingAudioRef.current = true;
    console.log("[AudioCall] Starting audio playback, queue has", audioQueueRef.current.length, "items");

    try {
      while (audioQueueRef.current.length > 0) {
        const buffer = audioQueueRef.current.shift();
        console.log("[AudioCall] Processing buffer, remaining:", audioQueueRef.current.length);
        if (!buffer) continue;

        console.log("[AudioCall] Decoding audio data...");
        const decoded = await audioContext.decodeAudioData(buffer.slice(0));
        console.log("[AudioCall] Audio decoded, duration:", decoded.duration);

        await new Promise<void>((resolve) => {
          console.log("[AudioCall] Creating and starting audio source");
          const source = audioContext.createBufferSource();
          source.buffer = decoded;
          source.connect(outputNode ?? audioContext.destination);
          currentSpeechSourceRef.current = source;
          source.onended = () => {
            console.log("[AudioCall] Audio source ended");
            resolve();
          };
          source.start(0);
        });
        currentSpeechSourceRef.current = null;
      }
    } catch (error) {
      console.error("[AudioCall] Error during audio playback:", error);
    } finally {
      isPlayingAudioRef.current = false;
      console.log("[AudioCall] Audio playback finished");
    }
  };

  const enqueueSpeechPlayback = async (text: string) => {
    console.log("[AudioCall] enqueueSpeechPlayback called with text:", text);
    const sentences = splitIntoSentences(text);
    console.log("[AudioCall] Split into sentences:", sentences);
    if (sentences.length === 0) {
      console.log("[AudioCall] No sentences to speak");
      return;
    }

    try {
      console.log("[AudioCall] Starting parallel TTS synthesis for", sentences.length, "sentences");

      // –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ TTS –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
      const audioBuffers = await Promise.all(
        sentences.map(async (sentence) => {
          try {
            return await psychologistAI.synthesizeSpeech(sentence);
          } catch (error) {
            console.warn("[AudioCall] Failed to synthesize sentence:", sentence, error);
            return null; // –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ—É–¥–∞—á–Ω—ã–µ —Å–∏–Ω—Ç–µ–∑—ã
          }
        })
      );

      // –§–∏–ª—å—Ç—Ä—É–µ–º —É—Å–ø–µ—à–Ω—ã–µ –±—É—Ñ–µ—Ä—ã
      const validBuffers = audioBuffers.filter(buffer => buffer !== null);
      console.log("[AudioCall] TTS completed, enqueuing", validBuffers.length, "valid audio buffers");

      if (validBuffers.length > 0) {
        audioQueueRef.current.push(...validBuffers);
        void playQueuedAudio();
        console.log("[AudioCall] Audio queued and playback started");
      }
    } catch (error) {
      console.error("[AudioCall] Error synthesizing speech:", error);
      setAudioError("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–∑–≤—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â—ë —Ä–∞–∑.");
    }
  };

  const stopAssistantSpeech = () => {
    audioQueueRef.current = [];
    if (currentSpeechSourceRef.current) {
      try {
        currentSpeechSourceRef.current.stop();
      } catch (error) {
        console.warn("Error stopping speech source:", error);
      }
      currentSpeechSourceRef.current.disconnect();
      currentSpeechSourceRef.current = null;
    }
    isPlayingAudioRef.current = false;
  };

  const startVolumeMonitoring = (stream: MediaStream) => {
    try {
      const audioContext = initializeAudioContext();
      const source = audioContext.createMediaStreamSource(stream);
      const analyser = audioContext.createAnalyser();
      analyser.fftSize = 256;
      source.connect(analyser);
      audioAnalyserRef.current = analyser;

      const dataArray = new Uint8Array(analyser.frequencyBinCount);
      const checkVolume = () => {
        if (!recognitionActiveRef.current || !audioAnalyserRef.current) {
          return;
        }
        analyser.getByteFrequencyData(dataArray);
        let sum = 0;
        for (let i = 0; i < dataArray.length; i++) {
          sum += dataArray[i];
        }
        const average = sum / dataArray.length;
        
        // –ï—Å–ª–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω –∑–≤—É–∫ –∏ –ú–∞—Ä–∫ –≥–æ–≤–æ—Ä–∏—Ç - –ø—Ä–µ—Ä—ã–≤–∞–µ–º –µ–≥–æ
        if (average > VOICE_DETECTION_THRESHOLD && isPlayingAudioRef.current) {
          console.debug(`[AudioCall] –û–±–Ω–∞—Ä—É–∂–µ–Ω –≥–æ–ª–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–≥—Ä–æ–º–∫–æ—Å—Ç—å: ${average.toFixed(1)}), –ø—Ä–µ—Ä—ã–≤–∞–µ–º –ú–∞—Ä–∫–∞`);
          stopAssistantSpeech();
        }

        volumeMonitorRef.current = window.requestAnimationFrame(checkVolume);
      };
      volumeMonitorRef.current = window.requestAnimationFrame(checkVolume);
    } catch (error) {
      console.warn("Error starting volume monitoring:", error);
    }
  };

  const stopVolumeMonitoring = () => {
    if (volumeMonitorRef.current) {
      cancelAnimationFrame(volumeMonitorRef.current);
      volumeMonitorRef.current = null;
    }
    if (audioAnalyserRef.current) {
      audioAnalyserRef.current.disconnect();
      audioAnalyserRef.current = null;
    }
  };

  const stopRecognition = () => {
    recognitionActiveRef.current = false;
    if (recognitionRef.current) {
      try {
        recognitionRef.current.onresult = null;
        recognitionRef.current.onerror = null;
        recognitionRef.current.onend = null;
        recognitionRef.current.stop();
      } catch (error) {
        console.warn("Error stopping speech recognition:", error);
      }
      recognitionRef.current = null;
    }
  };

  const cleanupRecording = () => {
    stopRecognition();
    stopVolumeMonitoring();
    resetAudioPlayback();
    conversationRef.current = [];
    responseQueueRef.current = Promise.resolve();
    pendingTranscriptRef.current = "";
    if (pendingProcessTimeoutRef.current) {
      window.clearTimeout(pendingProcessTimeoutRef.current);
      pendingProcessTimeoutRef.current = null;
    }

    if (audioStreamRef.current) {
      audioStreamRef.current.getTracks().forEach((track) => track.stop());
      audioStreamRef.current = null;
    }

    setTranscriptionStatus(null);
  };

  const processRecognizedText = async (rawText: string) => {
    console.log("[AudioCall] processRecognizedText –≤—ã–∑–≤–∞–Ω–∞ —Å —Ç–µ–∫—Å—Ç–æ–º:", rawText);

    if (!user) {
      console.warn("[AudioCall] –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É");
      return;
    }

    const text = rawText.trim();
    if (!text) {
      console.info("[AudioCall] –†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π ‚Äî –≤–æ–∑–º–æ–∂–Ω–æ, —Ç–∏—à–∏–Ω–∞ –∏–ª–∏ –Ω–µ—Ä–∞–∑–±–æ—Ä—á–∏–≤–∞—è —Ä–µ—á—å.");
      return;
    }

    console.info("[AudioCall] –†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:", text);
    setTranscriptionStatus("–ú–∞—Ä–∫ –æ–±–¥—É–º—ã–≤–∞–µ—Ç –æ—Ç–≤–µ—Ç...");

    conversationRef.current.push({ role: "user", content: text });

    try {
      // –ù–∞—á–∏–Ω–∞–µ–º –∑–≤—É–∫–æ–≤—ã–µ —Å–∏–≥–Ω–∞–ª—ã –≤–æ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞
      startProcessingSound();

      const assistantReply = await psychologistAI.getVoiceResponse(conversationRef.current, memoryRef.current, fastMode, callType === 'teacher');
      conversationRef.current.push({ role: "assistant", content: assistantReply });

      // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–∏–≥–Ω–∞–ª—ã –∏ –Ω–∞—á–∏–Ω–∞–µ–º TTS
      stopProcessingSound();
      setTranscriptionStatus("–û–∑–≤—É—á–∏–≤–∞—é –æ—Ç–≤–µ—Ç...");

      await enqueueSpeechPlayback(assistantReply);
      setTranscriptionStatus("");
      await updateConversationMemory(text, assistantReply);
    } catch (error) {
      console.error("Error generating assistant response:", error);
      stopProcessingSound(); // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–∏–≥–Ω–∞–ª—ã –ø—Ä–∏ –æ—à–∏–±–∫–µ
      setAudioError("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–∑–≤—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â—ë —Ä–∞–∑.");
      setTranscriptionStatus("");
    }
  };

  const flushPendingTranscript = () => {
    const text = pendingTranscriptRef.current.trim();
    pendingTranscriptRef.current = "";
    if (!text) {
      return;
    }

    stopAssistantSpeech();

    responseQueueRef.current = responseQueueRef.current
      .catch((error) => console.error("Previous voice response error:", error))
      .then(() => processRecognizedText(text));
  };

  const handleRecognizedText = (rawText: string) => {
    console.log("[AudioCall] handleRecognizedText called with:", rawText);
    const segment = rawText.trim();
    if (!segment) {
      console.log("[AudioCall] Empty segment, skipping");
      return;
    }

    console.log("[AudioCall] Stopping assistant speech before processing");
    stopAssistantSpeech();

    pendingTranscriptRef.current = [pendingTranscriptRef.current, segment].filter(Boolean).join(" ");
    console.log("[AudioCall] Updated pending transcript:", pendingTranscriptRef.current);

    if (pendingProcessTimeoutRef.current) {
      window.clearTimeout(pendingProcessTimeoutRef.current);
    }

    const timeoutDelay = fastMode ? 50 : 100; // –í –±—ã—Å—Ç—Ä–æ–º —Ä–µ–∂–∏–º–µ - 50–º—Å, –≤ –æ–±—ã—á–Ω–æ–º - 100–º—Å
    pendingProcessTimeoutRef.current = window.setTimeout(() => {
      pendingProcessTimeoutRef.current = null;
      console.log("[AudioCall] Timeout reached, flushing pending transcript");
      flushPendingTranscript();
    }, timeoutDelay);
  };

  const updateConversationMemory = async (userText: string, assistantText: string) => {
    if (!user) {
      return;
    }

    const entry = `–ö–ª–∏–µ–Ω—Ç: ${userText}\n–ú–∞—Ä–∫: ${assistantText}`;
    try {
      const updatedMemory = await memoryApi.appendMemory(user.id, "audio", entry);
      memoryRef.current = updatedMemory;
    } catch (error) {
      console.error("Error updating audio memory:", error);
    }
  };

  const getUserCredentials = () => {
    const fallbackEmail = 'user@zenmindmate.com';
    const email = authUser?.email ?? fallbackEmail;
    const name = authUser?.name ?? authUser?.email ?? '–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å';
    return { email, name };
  };

  // Default user ID for demo purposes
  useEffect(() => {
    initializeUser();
  }, [authUser]);

  useEffect(() => {
    isMutedRef.current = isMuted;

    const stream = audioStreamRef.current;
    if (!stream) {
      if (isMuted) {
        setTranscriptionStatus("–ú–∏–∫—Ä–æ—Ñ–æ–Ω –≤—ã–∫–ª—é—á–µ–Ω");
      }
      return;
    }

    stream.getAudioTracks().forEach((track) => {
      track.enabled = !isMuted;
    });

    if (isMuted) {
      setTranscriptionStatus("–ú–∏–∫—Ä–æ—Ñ–æ–Ω –≤—ã–∫–ª—é—á–µ–Ω");
      if (recognitionRef.current) {
        try {
          recognitionRef.current.stop();
        } catch (error) {
          console.warn("–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è:", error);
        }
      }
    } else if (recognitionActiveRef.current && recognitionRef.current) {
      setTranscriptionStatus("");
      try {
        recognitionRef.current.start();
      } catch (error) {
        // –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –º–æ–≥–ª–æ —É–∂–µ —Ä–∞–±–æ—Ç–∞—Ç—å ‚Äì –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫—É —Å–æ—Å—Ç–æ—è–Ω–∏—è
        if (error instanceof DOMException && error.name === "InvalidStateError") {
          return;
        }
        console.warn("–ù–µ —É–¥–∞–ª–æ—Å—å –≤–æ–∑–æ–±–Ω–æ–≤–∏—Ç—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏:", error);
      }
    }
  }, [isMuted]);

  useEffect(() => {
    isSpeakerOnRef.current = isSpeakerOn;

    if (speakerGainRef.current && audioContextRef.current) {
      const gain = speakerGainRef.current.gain;
      gain.setValueAtTime(isSpeakerOn ? 1 : 0, audioContextRef.current.currentTime ?? 0);
    }

    if (backgroundMusicRef.current.audioElement) {
      backgroundMusicRef.current.audioElement.muted = !isSpeakerOn;
    }
  }, [isSpeakerOn]);

  useEffect(() => {
    return () => {
      cleanupRecording();
      stopProcessingSound(); // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–≤—É–∫–æ–≤—ã–µ —Å–∏–≥–Ω–∞–ª—ã –ø—Ä–∏ —Ä–∞–∑–º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏
      // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–æ–Ω–æ–≤—É—é –º—É–∑—ã–∫—É –ø—Ä–∏ —Ä–∞–∑–º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏
      stopBackgroundMusic();
      if (callTimerRef.current) {
        clearInterval(callTimerRef.current);
      }
      if (pendingProcessTimeoutRef.current) {
        window.clearTimeout(pendingProcessTimeoutRef.current);
        pendingProcessTimeoutRef.current = null;
      }
    };
  }, []);

  const initializeUser = async () => {
    try {
      const { email, name } = getUserCredentials();
      const userData = await userApi.getOrCreateUser(email, name);
      setUser(userData);
      const info = await subscriptionApi.getAudioSessionInfo(userData.id);
      setSubscriptionInfo(info);
    } catch (error) {
      console.error('Error initializing user:', error);
    } finally {
      setLoading(false);
    }
  };

  const startCall = async () => {
    console.log("[AudioCall] –ù–∞—á–∏–Ω–∞–µ–º –∑–∞–ø—É—Å–∫ –∑–≤–æ–Ω–∫–∞...");

    if (!user || isCallActive) {
      console.log("[AudioCall] –ó–≤–æ–Ω–æ–∫ —É–∂–µ –∞–∫—Ç–∏–≤–µ–Ω –∏–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω");
      return;
    }

    if (typeof navigator === "undefined" || !navigator.mediaDevices) {
      console.error("[AudioCall] –ë—Ä–∞—É–∑–µ—Ä –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç mediaDevices");
      setAudioError("–í–∞—à –±—Ä–∞—É–∑–µ—Ä –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∑–∞–ø–∏—Å—å –∞—É–¥–∏–æ.");
      return;
    }

    console.log("[AudioCall] –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫—É Speech Recognition...");

    try {
      setAudioError(null);
      setIsMuted(false);
      isMutedRef.current = false;
      setIsSpeakerOn(true);
      setCallDuration(0);
      callLimitReachedRef.current = false;
      callLimitWarningSentRef.current = false;
      memoryRef.current = await memoryApi.getMemory(user.id, "audio");

      const sessionInfo = await subscriptionApi.getAudioSessionInfo(user.id);
      setSubscriptionInfo(sessionInfo);

      if (sessionInfo.plan === 'premium') {
        if (sessionInfo.status !== 'active') {
          setAudioError('–í–∞—à–∞ –ø—Ä–µ–º–∏—É–º –ø–æ–¥–ø–∏—Å–∫–∞ –Ω–µ –∞–∫—Ç–∏–≤–Ω–∞. –ü—Ä–æ–¥–ª–∏—Ç–µ –ø–æ–¥–ø–∏—Å–∫—É, —á—Ç–æ–±—ã –¥–µ–ª–∞—Ç—å –∑–≤–æ–Ω–∫–∏.');
          setIsCallActive(false);
          return;
        }

        if (sessionInfo.remaining <= 0) {
          setAudioError('–õ–∏–º–∏—Ç –∞—É–¥–∏–æ —Å–µ—Å—Å–∏–π –Ω–∞ —ç—Ç–æ—Ç –º–µ—Å—è—Ü –∏—Å—á–µ—Ä–ø–∞–Ω.');
          setIsCallActive(false);
          return;
        }
      }

      const SpeechRecognitionConstructor =
        typeof window !== "undefined"
          ? (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition
          : null;

      console.log("[AudioCall] SpeechRecognition constructor:", SpeechRecognitionConstructor);

      if (!SpeechRecognitionConstructor) {
        console.error("[AudioCall] Speech Recognition API –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è");
        setAudioError("–í–∞—à –±—Ä–∞—É–∑–µ—Ä –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Å–∏—Å—Ç–µ–º–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏.");
        return;
      }

      console.log("[AudioCall] –°–æ–∑–¥–∞–µ–º –∞—É–¥–∏–æ —Å–µ—Å—Å–∏—é –≤ –ë–î...");

      const call = await audioCallApi.createAudioCall(user.id);
      setCurrentCallId(call.id);
      console.log("[AudioCall] –ê—É–¥–∏–æ —Å–µ—Å—Å–∏—è —Å–æ–∑–¥–∞–Ω–∞, ID:", call.id);

      console.log("[AudioCall] –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –¥–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É...");
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      audioStreamRef.current = stream;
      console.log("[AudioCall] –ú–∏–∫—Ä–æ—Ñ–æ–Ω –¥–æ—Å—Ç—É–ø–µ–Ω, —Å–æ–∑–¥–∞–µ–º recognition...");

      const recognition = new SpeechRecognitionConstructor();
      recognition.lang = "ru-RU";
      recognition.continuous = true;
      recognition.interimResults = false;
      recognition.maxAlternatives = 1;

      recognition.onresult = (event: any) => {
        console.log("[AudioCall] Recognition result event:", event);
        for (let i = event.resultIndex; i < event.results.length; i += 1) {
          const result = event.results[i];
          const transcript = result?.[0]?.transcript ?? "";
          console.log(`[AudioCall] Result ${i}: final=${result.isFinal}, transcript="${transcript}"`);
          if (result.isFinal && transcript) {
            console.log("[AudioCall] –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –ø–µ—Ä–µ–¥–∞–µ–º –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É");
            handleRecognizedText(transcript);
          }
        }
      };

      recognition.onspeechstart = () => {
        console.log("[AudioCall] Speech started - stopping assistant speech");
        stopAssistantSpeech();
      };

      recognition.onaudiostart = () => {
        console.log("[AudioCall] Audio started - stopping assistant speech");
        stopAssistantSpeech();
      };

      recognition.onerror = (event: any) => {
        console.error("[AudioCall] Speech recognition error:", event);
        if (event?.error === "not-allowed" || event?.error === "service-not-allowed") {
          setAudioError("–î–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É –∑–∞–ø—Ä–µ—â—ë–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –±—Ä–∞—É–∑–µ—Ä–∞.");
        } else if (event?.error !== "aborted") {
          setAudioError("–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â—ë —Ä–∞–∑.");
        }
      };

      recognition.onend = () => {
        if (!recognitionActiveRef.current) {
          return;
        }

        if (isMutedRef.current) {
          setTranscriptionStatus("–ú–∏–∫—Ä–æ—Ñ–æ–Ω –≤—ã–∫–ª—é—á–µ–Ω");
          return;
        }

        try {
          recognition.start();
        } catch (error) {
          console.warn("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏:", error);
        }
      };

      recognitionRef.current = recognition;
      recognitionActiveRef.current = true;
      console.log("[AudioCall] Recognition —Å–æ–∑–¥–∞–Ω –∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω");

      // –ó–∞–ø—É—Å–∫–∞–µ–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≥—Ä–æ–º–∫–æ—Å—Ç–∏ –¥–ª—è –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è –ú–∞—Ä–∫–∞
      startVolumeMonitoring(stream);
      console.log("[AudioCall] –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≥—Ä–æ–º–∫–æ—Å—Ç–∏ –∑–∞–ø—É—â–µ–Ω");

      try {
        console.log("[AudioCall] –ó–∞–ø—É—Å–∫–∞–µ–º speech recognition...");
        recognition.start();
        console.log("[AudioCall] Speech recognition –∑–∞–ø—É—â–µ–Ω —É—Å–ø–µ—à–Ω–æ");
      } catch (error) {
        console.error("[AudioCall] Failed to start speech recognition:", error);
        setAudioError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏.");
        stopRecognition();
        cleanupRecording();
        await audioCallApi.endAudioCall(call.id, 0);
        setCurrentCallId(null);
        return;
      }

      console.log("[AudioCall] –ü—Ä–æ–∏–≥—Ä—ã–≤–∞–µ–º –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ...");
      const greeting = callType === 'psychologist'
        ? "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ. –Ø –ú–∞—Ä–∫, –ø—Å–∏—Ö–æ–ª–æ–≥. –†–∞—Å—Å–∫–∞–∂–∏—Ç–µ, —á—Ç–æ –≤–∞—Å —Å–µ–π—á–∞—Å –±–æ–ª—å—à–µ –≤—Å–µ–≥–æ –±–µ—Å–ø–æ–∫–æ–∏—Ç?"
        : "–ü—Ä–∏–≤–µ—Ç! –Ø –ê–ª–µ–∫—Å–µ–π, —Ç–≤–æ–π —É—á–∏—Ç–µ–ª—å —Ñ–∏–∑–∏–∫–∏. –†–∞—Å—Å–∫–∞–∂–∏, —Å —á–µ–º —Ç–µ–±–µ –Ω—É–∂–Ω–∞ –ø–æ–º–æ—â—å –ø–æ —Ñ–∏–∑–∏–∫–µ?";
      conversationRef.current.push({ role: "assistant", content: greeting });
      await enqueueSpeechPlayback(greeting);
      console.log("[AudioCall] –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ –ø—Ä–æ–∏–≥—Ä–∞–Ω–æ");

      if (callTimerRef.current) {
        clearInterval(callTimerRef.current);
      }

      callTimerRef.current = window.setInterval(() => {
        setCallDuration((prev) => {
          const next = prev + 1;
          if (!callLimitWarningSentRef.current && next >= CALL_LIMIT_WARNING_SECONDS && next < MAX_CALL_DURATION_SECONDS) {
            callLimitWarningSentRef.current = true;
            const warningMessage = "–£ –Ω–∞—Å –æ—Å—Ç–∞–ª–æ—Å—å –æ–∫–æ–ª–æ –ø—è—Ç–∏ –º–∏–Ω—É—Ç. –î–∞–≤–∞–π—Ç–µ –∫–æ—Ä–æ—Ç–∫–æ –≤—Å–ø–æ–º–Ω–∏–º, —á—Ç–æ –≤—ã —É—Å–ø–µ–ª–∏ –ø—Ä–æ–≥–æ–≤–æ—Ä–∏—Ç—å –∏ —á—Ç–æ —Ö–æ—Ç–∏—Ç–µ –∑–∞–±—Ä–∞—Ç—å —Å —Å–æ–±–æ–π";
            conversationRef.current.push({ role: "assistant", content: warningMessage });
            responseQueueRef.current = responseQueueRef.current
              .catch((error) => console.error("Previous voice response error:", error))
              .then(async () => {
                try {
                  setTranscriptionStatus("–û–∑–≤—É—á–∏–≤–∞—é –Ω–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ...");
                  await enqueueSpeechPlayback(warningMessage);
                } catch (error) {
                  console.error("Error playing limit warning:", error);
                } finally {
                  setTranscriptionStatus("");
                }
              });
          }
          if (next >= MAX_CALL_DURATION_SECONDS && !callLimitReachedRef.current) {
            callLimitReachedRef.current = true;
            window.setTimeout(() => {
              setAudioError("–ó–≤–æ–Ω–æ–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≤–µ—Ä—à—ë–Ω: –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç 40 –º–∏–Ω—É—Ç.");
              endCall(next).catch((error) => console.error("Error auto-ending call", error));
            }, 0);
            return MAX_CALL_DURATION_SECONDS;
          }
          return Math.min(next, MAX_CALL_DURATION_SECONDS);
        });
      }, 1000);

      setTranscriptionStatus("");
      setIsCallActive(true);

      // –ó–∞–ø—É—Å–∫–∞–µ–º —Ñ–æ–Ω–æ–≤—É—é –º—É–∑—ã–∫—É –∏–∑ —Ñ–∞–π–ª–∞
      startBackgroundMusic();
    } catch (error) {
      console.error("Error starting call:", error);
      setAudioError("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.");
      cleanupRecording();
      setCurrentCallId(null);
    }
  };

  const endCall = async (overrideDuration?: number) => {
    if (!currentCallId) {
      return;
    }

    try {
      cleanupRecording();
      stopProcessingSound(); // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–≤—É–∫–æ–≤—ã–µ —Å–∏–≥–Ω–∞–ª—ã

      if (callTimerRef.current) {
        clearInterval(callTimerRef.current);
        callTimerRef.current = null;
      }

      const durationToSave = overrideDuration ?? callDuration;
      await audioCallApi.endAudioCall(currentCallId, durationToSave);

      if (user && durationToSave > 0 && subscriptionInfo?.plan === 'premium' && subscriptionInfo.status === 'active') {
        const result = await subscriptionApi.recordAudioSession(user.id);
        if (!result.success && result.message) {
          setAudioError(result.message);
        }
        const latestInfo = await subscriptionApi.getAudioSessionInfo(user.id);
        setSubscriptionInfo(latestInfo);
      }
    } catch (error) {
      console.error("Error ending call:", error);
      setAudioError("–ù–µ —É–¥–∞–ª–æ—Å—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∑–∞–≤–µ—Ä—à–∏—Ç—å –∑–≤–æ–Ω–æ–∫.");
    } finally {
      // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–æ–Ω–æ–≤—É—é –º—É–∑—ã–∫—É
      stopBackgroundMusic();
      stopAssistantSpeech();
      setIsCallActive(false);
      setCallDuration(0);
      setIsMuted(false);
      isMutedRef.current = false;
      setCurrentCallId(null);
      setTranscriptionStatus(null);
      callLimitReachedRef.current = false;
      callLimitWarningSentRef.current = false;
      memoryRef.current = "";
    }
  };

  const formatDuration = (seconds: number) => {
    const mins = Math.floor(seconds / 60);
    const secs = seconds % 60;
    return `${mins.toString().padStart(2, "0")}:${secs.toString().padStart(2, "0")}`;
  };

  return (
    <div className="min-h-screen bg-calm-gradient">
      <Navigation />
      
      <div className="pt-24 pb-8 px-4">
        <div className="container mx-auto max-w-2xl">
          <div className="text-center mb-8 animate-fade-in">
            <h1 className="text-4xl font-bold text-foreground mb-3">
              ü§ñ {callType === 'psychologist' ? '–ì–æ–ª–æ—Å–æ–≤–æ–π –∑–≤–æ–Ω–æ–∫ —Å –ò–ò' : 'üìû –ó–≤–æ–Ω–æ–∫ —Å —É—á–∏—Ç–µ–ª–µ–º'}
            </h1>
            <p className="text-muted-foreground">
              {callType === 'psychologist' ? '–ì–æ–ª–æ—Å–æ–≤–∞—è —Å–µ—Å—Å–∏—è —Å –ò–ò-–ø—Å–∏—Ö–æ–ª–æ–≥–æ–º' : '–ü–æ–º–æ—â—å —Å –¥–æ–º–∞—à–Ω–∏–º–∏ –∑–∞–¥–∞–Ω–∏—è–º–∏'}
            </p>

            {callType === 'teacher' && (
              <div className="mt-6 p-4 bg-muted/50 rounded-lg">
                <h3 className="text-lg font-semibold text-foreground mb-2">–¢–µ–∫—É—â–∏–π —É—Ä–æ–∫: –§–∏–∑–∏–∫–∞ –¥–ª—è 6 –∫–ª–∞—Å—Å–∞</h3>
                <p className="text-sm text-muted-foreground">
                  –ö—É—Ä—Å —Ñ–∏–∑–∏–∫–∏ –¥–ª—è —à–µ—Å—Ç–∏–∫–ª–∞—Å—Å–Ω–∏–∫–æ–≤ —Å —É–≥–ª—É–±–ª–µ–Ω–∏–µ–º –≤ –º–µ—Ö–∞–Ω–∏–∫—É, –∏–∑—É—á–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã, —ç–Ω–µ—Ä–≥–∏–∏, –ø—Ä–æ—Å—Ç—ã—Ö –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –∏ —Ç–µ–ø–ª–æ–≤—ã—Ö —è–≤–ª–µ–Ω–∏–π. –†–∞–∑–≤–∏—Ç–∏–µ –Ω–∞–≤—ã–∫–æ–≤ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –∑–∞–∫–æ–Ω–æ–≤.
                </p>
              </div>
            )}
          </div>

          {/* –í—ã–±–æ—Ä —Ç–∏–ø–∞ –∑–≤–æ–Ω–∫–∞ */}
          <div className="flex justify-center gap-4 mb-6">
            <Button
              onClick={() => setCallType('psychologist')}
              variant={callType === 'psychologist' ? 'default' : 'outline'}
              className={`px-6 py-2 ${callType === 'psychologist' ? 'bg-hero-gradient text-white' : ''}`}
            >
              ü§ñ –ü—Å–∏—Ö–æ–ª–æ–≥
            </Button>
            <Button
              onClick={() => setCallType('teacher')}
              variant={callType === 'teacher' ? 'default' : 'outline'}
              className={`px-6 py-2 ${callType === 'teacher' ? 'bg-blue-500 text-white' : ''}`}
            >
              üìû –£—á–∏—Ç–µ–ª—å
            </Button>
          </div>

          <Card className="bg-card-gradient border-2 border-border shadow-strong p-8 md:p-12 text-center animate-scale-in">
            {!isCallActive ? (
              <div className="space-y-8">
                <div className="w-32 h-32 mx-auto rounded-full bg-hero-gradient text-white flex items-center justify-center shadow-strong animate-pulse-soft">
                  <Phone className="w-16 h-16 " />
                </div>
                
                <div>
                  <h2 className="text-2xl font-bold text-foreground mb-2">
                    {callType === 'psychologist' ? '–ù–∞—á–∞—Ç—å –∑–≤–æ–Ω–æ–∫ —Å –ø—Å–∏—Ö–æ–ª–æ–≥–æ–º' : '–ù–∞—á–∞—Ç—å –∑–≤–æ–Ω–æ–∫ —Å —É—á–∏—Ç–µ–ª–µ–º'}
                  </h2>
                  <p className="text-muted-foreground">
                    {callType === 'psychologist'
                      ? '–ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É –Ω–∏–∂–µ, —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å –≥–æ–ª–æ—Å–æ–≤—É—é —Å–µ—Å—Å–∏—é'
                      : '–ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É –Ω–∏–∂–µ, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –ø–æ–º–æ—â—å —Å —É—Ä–æ–∫–∞–º–∏'
                    }
                  </p>
                  {subscriptionInfo && subscriptionInfo.plan === 'premium' ? (
                    <p className="mt-3 text-sm text-primary font-medium">
                      –û—Å—Ç–∞–ª–æ—Å—å –∞—É–¥–∏–æ —Å–µ—Å—Å–∏–π: {subscriptionInfo.remaining} –∏–∑ {subscriptionInfo.limit}
                    </p>
                  ) : subscriptionInfo ? (
                    <p className="mt-3 text-sm text-muted-foreground">
                      –û—Å—Ç–∞–ª–æ—Å—å –∞—É–¥–∏–æ —Å–µ—Å—Å–∏–π: {subscriptionInfo.remaining} –∏–∑ {subscriptionInfo.limit}
                    </p>
                  ) : null}
                </div>

                <Button
                  onClick={startCall}
                  size="lg"
                  className="bg-hero-gradient text-white hover:shadow-lg  shadow-medium text-lg px-12 py-6"
                  disabled={loading}
                >
                  <Phone className="w-6 h-6 mr-2" />
                  {loading ? "–ó–∞–≥—Ä—É–∑–∫–∞..." : "–ü–æ–∑–≤–æ–Ω–∏—Ç—å"}
                </Button>

                <div className="pt-8 border-t border-border">
                  <h3 className="text-lg font-semibold text-foreground mb-4">–°–æ–≤–µ—Ç—ã –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ–≥–æ –∑–≤–æ–Ω–∫–∞:</h3>
                  <ul className="text-left text-muted-foreground space-y-2 max-w-md mx-auto">
                    <li>‚Ä¢ –ù–∞–π–¥–∏—Ç–µ —Ç–∏—Ö–æ–µ –º–µ—Å—Ç–æ, –≥–¥–µ –≤–∞—Å –Ω–∏–∫—Ç–æ –Ω–µ –ø–æ–±–µ—Å–ø–æ–∫–æ–∏—Ç</li>
                    <li>‚Ä¢ –ü–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ —Ç–µ–º—ã, –∫–æ—Ç–æ—Ä—ã–µ —Ö–æ—Ç–∏—Ç–µ –æ–±—Å—É–¥–∏—Ç—å</li>
                    <li>‚Ä¢ –ì–æ–≤–æ—Ä–∏—Ç–µ –æ—Ç–∫—Ä—ã—Ç–æ –∏ —á–µ—Å—Ç–Ω–æ</li>
                    <li>‚Ä¢ –ù–µ —Ç–æ—Ä–æ–ø–∏—Ç–µ—Å—å, –¥–∞–π—Ç–µ —Å–µ–±–µ –≤—Ä–µ–º—è –ø–æ–¥—É–º–∞—Ç—å</li>
                  </ul>
                </div>
              </div>
            ) : (
              <div className="space-y-8">
                <div className="w-40 h-40 mx-auto rounded-full bg-green-500/20 border-4 border-green-500 flex items-center justify-center shadow-strong animate-pulse">
                  <div className="w-32 h-32 rounded-full bg-green-500 flex items-center justify-center">
                    <PhoneOff className="w-16 h-16 text-white animate-pulse" />
                  </div>
                </div>

                <div>
                  <h2 className="text-2xl font-bold text-foreground mb-2">
                    {callType === 'psychologist' ? '–°–ª—É—à–∞—é –≤–∞—Å...' : '–ì–æ—Ç–æ–≤ –ø–æ–º–æ—á—å —Å —É—Ä–æ–∫–æ–º!'}
                  </h2>
                  <p className="text-muted-foreground">
                    {callType === 'psychologist' ? '–ó–∞–≤–µ—Ä—à–∏—Ç—å –∑–≤–æ–Ω–æ–∫' : '–ó–∞–≤–µ—Ä—à–∏—Ç—å —É—Ä–æ–∫'}
                  </p>
                </div>

                <div className="flex justify-center gap-4">
                  <Button
                    onClick={() => setIsMuted(!isMuted)}
                    size="lg"
                    variant={isMuted ? "destructive" : "outline"}
                    className="rounded-full w-16 h-16 p-0"
                  >
                    {isMuted ? <MicOff className="w-6 h-6" /> : <Mic className="w-6 h-6" />}
                  </Button>

                  <Button
                    onClick={() => setIsSpeakerOn(!isSpeakerOn)}
                    size="lg"
                    variant={!isSpeakerOn ? "destructive" : "outline"}
                    className="rounded-full w-16 h-16 p-0"
                  >
                    {isSpeakerOn ? <Volume2 className="w-6 h-6" /> : <VolumeX className="w-6 h-6" />}
                  </Button>

                  <Button
                    onClick={() => setFastMode(!fastMode)}
                    size="lg"
                    variant={fastMode ? "default" : "outline"}
                    className="rounded-full w-16 h-16 p-0"
                    title={fastMode ? "–í—ã–∫–ª—é—á–∏—Ç—å –±—ã—Å—Ç—Ä—ã–π —Ä–µ–∂–∏–º" : "–í–∫–ª—é—á–∏—Ç—å –±—ã—Å—Ç—Ä—ã–π —Ä–µ–∂–∏–º"}
                  >
                    ‚ö°
                  </Button>

                  <Button
                    onClick={endCall}
                    size="lg"
                    variant="destructive"
                    className="rounded-full w-16 h-16 p-0 shadow-medium"
                  >
                    <PhoneOff className="w-6 h-6" />
                  </Button>
                </div>

                <div className="text-center text-sm text-muted-foreground">
                  {!isSpeakerOn && <p>–ó–≤—É–∫ –≤—ã–∫–ª—é—á–µ–Ω</p>}
                  {fastMode && <p className="text-primary font-medium">‚ö° –ë—ã—Å—Ç—Ä—ã–π —Ä–µ–∂–∏–º –∞–∫—Ç–∏–≤–µ–Ω</p>}
                </div>

                {subscriptionInfo && (
                  <p className="text-xs text-muted-foreground">
                    –û—Å—Ç–∞–ª–æ—Å—å —Å–µ—Å—Å–∏–π –≤ —ç—Ç–æ–º –º–µ—Å—è—Ü–µ: {subscriptionInfo.remaining} –∏–∑ {subscriptionInfo.limit}
                  </p>
                )}

                {transcriptionStatus && (
                  <div className="flex items-center gap-2">
                    <p className="text-sm text-primary/80">{transcriptionStatus}</p>
                    {transcriptionStatus.includes("–æ–±–¥—É–º—ã–≤–∞–µ—Ç") && (
                      <Button
                        onClick={() => {
                          // –ü—Ä–µ—Ä—ã–≤–∞–µ–º –æ–∂–∏–¥–∞–Ω–∏–µ –∏ –æ—á–∏—â–∞–µ–º —Å—Ç–∞—Ç—É—Å
                          if (pendingProcessTimeoutRef.current) {
                            window.clearTimeout(pendingProcessTimeoutRef.current);
                            pendingProcessTimeoutRef.current = null;
                          }
                          setTranscriptionStatus("");
                          console.log("[AudioCall] User skipped waiting for response");
                        }}
                        size="sm"
                        variant="outline"
                        className="text-xs px-2 py-1"
                      >
                        –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å
                      </Button>
                    )}
                  </div>
                )}

                {audioError && (
                  <p className="text-sm text-destructive">{audioError}</p>
                )}
              </div>
            )}
          </Card>
        </div>
      </div>
    </div>
  );
};

export default AudioCall;
